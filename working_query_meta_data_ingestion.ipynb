{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Filtering Records with Focus (Primary or Secondary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 85.42% of entire records are Primary or Secondary \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"Data/meta_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "# Filter JSON entries where focus is primary or secondary\n",
    "filtered_records = [entry for entry in records if any(f in [\"primary\", \"secondary\"] for f in entry[\"metadata\"].get(\"focus\", []))]\n",
    "\n",
    "# Calculation of Primary and Secondary records %\n",
    "filtered_records_percent = round(((len(filtered_records)/len(records)) * 100), 2)\n",
    "\n",
    "print(f\"Only {filtered_records_percent}% of entire records are Primary or Secondary \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Restructuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_roots(records):\n",
    "    roots = set()\n",
    "    for r in records:\n",
    "        md = r.get(\"metadata\", {})\n",
    "        rn = md.get(\"root_name\")\n",
    "        if rn:\n",
    "            roots.add(str(rn).strip())\n",
    "    return sorted(roots)\n",
    "\n",
    "def build_alias_map(records):\n",
    "    alias2root = {}\n",
    "    for r in records:\n",
    "        md = r.get(\"metadata\", {})\n",
    "        root = str(md.get(\"root_name\") or \"\").strip().lower()\n",
    "        if not root:\n",
    "            continue\n",
    "        # map root to itself\n",
    "        alias2root[root] = root\n",
    "        # map synonyms to root\n",
    "        for s in md.get(\"synonyms\", []) or []:\n",
    "            alias2root[str(s).strip().lower()] = root\n",
    "    return alias2root\n",
    "\n",
    "def metadata_restructuring(records):\n",
    "    restructured_records = []\n",
    "    for record in records:\n",
    "        metadata = record.get(\"metadata\", {}).copy()  # copy to avoid mutating original\n",
    "\n",
    "        # Explicitly ensure top-level fields are part of metadata\n",
    "        for field in [\"root_name\", \"search_term\", \"synonyms\", \"PMID\", \"pubmed_type\"]:\n",
    "            if field in record:\n",
    "                metadata[field] = record[field]\n",
    "        \n",
    "        restructured_records.append({\"metadata\": metadata})\n",
    "    return restructured_records\n",
    "\n",
    "restructured_records = metadata_restructuring(filtered_records)\n",
    "\n",
    "ALIAS2ROOT = build_alias_map(restructured_records) \n",
    "ALL_ROOTS  = sorted(set(ALIAS2ROOT.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Validation Checkpoint to get matching record from json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_record_by_pmid(json_list, pmid):\n",
    "#     \"\"\"Pass PMID and get matching record from json_list\"\"\"\n",
    "#     for record in json_list:\n",
    "#         if record['metadata']['PMID'] == pmid:\n",
    "#             return record\n",
    "#     return None\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# result = get_record_by_pmid(restructured_records, 11524119)\n",
    "\n",
    "# if result:\n",
    "#     print(json.dumps(result, indent=2))  # Prints the entire matching record\n",
    "# else:\n",
    "#     print(\"PMID not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Flattening the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in restructured_records:\n",
    "    metadata = record[\"metadata\"]\n",
    "    \n",
    "    # Process interventions with Parallel - Indexing\n",
    "    interventions = metadata.get(\"interventions\", [])\n",
    "    record[\"intervention_names\"] = [i.get(\"ingredient\") for i in interventions]\n",
    "    record[\"intervention_dosages\"] = [i.get(\"daily_dosage\") for i in interventions]\n",
    "    record[\"intervention_units\"] = [i.get(\"units\") if i.get(\"units\") else \"\" for i in interventions]\n",
    "    record[\"intervention_original_texts\"] = [i.get(\"original_text\") for i in interventions]\n",
    "    \n",
    "    # Process outcomes with Parallel - Indexing\n",
    "    outcomes = metadata.get(\"outcomes\", [])\n",
    "    record[\"biomarker_names\"] = [o[\"name\"] for o in outcomes if o[\"domain\"] == \"biomarker\"]\n",
    "    record[\"biomarker_types\"] = [o[\"type\"] for o in outcomes if o[\"domain\"] == \"biomarker\"]\n",
    "    record[\"biomarker_results\"] = [o[\"result\"] for o in outcomes if o[\"domain\"] == \"biomarker\"]\n",
    "\n",
    "    record[\"function_names\"] = [o[\"name\"] for o in outcomes if o[\"domain\"] == \"function\"]\n",
    "    record[\"function_types\"] = [o[\"type\"] for o in outcomes if o[\"domain\"] == \"function\"]\n",
    "    record[\"function_results\"] = [o[\"result\"] for o in outcomes if o[\"domain\"] == \"function\"]\n",
    "\n",
    "    record[\"condition_names\"] = [o[\"name\"] for o in outcomes if o[\"domain\"] == \"condition\"]\n",
    "    record[\"condition_types\"] = [o[\"type\"] for o in outcomes if o[\"domain\"] == \"condition\"]\n",
    "    record[\"condition_results\"] = [o[\"result\"] for o in outcomes if o[\"domain\"] == \"condition\"]\n",
    "\n",
    "    # force consistent types for filtering\n",
    "    if \"published_year\" in metadata and isinstance(metadata[\"published_year\"], str) and metadata[\"published_year\"].isdigit():\n",
    "        metadata[\"published_year\"] = int(metadata[\"published_year\"])\n",
    "    if \"PMID\" in metadata:\n",
    "        metadata[\"PMID\"] = str(metadata[\"PMID\"])\n",
    "    \n",
    "    # Lowercase/canonicalize list fields (beyond synonyms)\n",
    "    for key in (\"study_type\", \"species\", \"experimental_model\", \"usage\",\n",
    "                \"keywords\", \"benefits\", \"diseases\", \"symptoms\", \"sample_gender\"):\n",
    "        if key in metadata and isinstance(metadata[key], list):\n",
    "            metadata[key] = [str(x).strip().lower() for x in metadata[key] if x is not None and str(x).strip()]\n",
    "\n",
    "    # Lowercase single-string fields you might filter on:\n",
    "    if isinstance(metadata.get(\"population\"), str):\n",
    "        metadata[\"population\"] = metadata[\"population\"].strip().lower()\n",
    "    if isinstance(metadata.get(\"location\"), str):\n",
    "        metadata[\"location\"] = metadata[\"location\"].strip().lower()\n",
    "\n",
    "    # Keep your synonyms normalization after this:\n",
    "    syns = metadata.get(\"synonyms\") or []\n",
    "    if isinstance(syns, list):\n",
    "        syns = sorted({str(x).strip().lower() for x in syns if x is not None and str(x).strip()})\n",
    "    else:\n",
    "        syns = []\n",
    "    metadata[\"synonyms\"] = syns\n",
    "\n",
    "    if metadata.get(\"root_name\"):\n",
    "        rn = str(metadata[\"root_name\"]).strip().lower()\n",
    "        if rn and rn not in metadata[\"synonyms\"]:\n",
    "            metadata[\"synonyms\"].append(rn)\n",
    "            \n",
    "    # Delete original detailed fields\n",
    "    for key in [\"interventions\", \"outcomes\", \"biomarkers\", \"functions\", \"conditions\"]:\n",
    "        metadata.pop(key, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/flatten.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(restructured_records, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Ingestion into PineCone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Converting into Embeddings and performing Sematic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \\\n",
    "#   pandas \\\n",
    "#   \"llama-index\" \\\n",
    "#   \"llama-index-embeddings-huggingface\" \\\n",
    "#   \"llama-index-vector-stores-p\"inecone\" \\\n",
    "#   \"llama-index-retrievers-bm25\" \\\n",
    "#   pinecone-client \\\n",
    "#   \"sentence-transformers\" \\\n",
    "#   transformers \\\n",
    "#   \"torch\" \\\n",
    "#   python-dotenv \\\n",
    "#   tqdm \\\n",
    "#     biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 82/82 [00:49<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from UPDATED_meta_data_generation import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --------------------------\n",
    "# Initialize Pinecone\n",
    "# --------------------------\n",
    "INDEX_NAME = \"pubmed-abstracts-v5\"\n",
    "client = Pinecone(api_key=os.getenv(\"PINECONE_API\"))\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "\n",
    "if INDEX_NAME not in client.list_indexes().names():\n",
    "    client.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "pinecone_index = client.Index(INDEX_NAME)\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "# --------------------------\n",
    "# Initialize embedding + semantic chunker\n",
    "# --------------------------\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Build all semantic nodes\n",
    "# --------------------------\n",
    "all_nodes = []\n",
    "\n",
    "for idx, row in enumerate(tqdm(restructured_records, desc=\"Processing papers\")):\n",
    "    md = row[\"metadata\"]\n",
    "    paper = fetch_extract_and_abstract(md['PMID'])\n",
    "    title = paper['title']\n",
    "    abstract = paper['abstract']\n",
    "\n",
    "    # Title node\n",
    "    title_node = Document(\n",
    "        text=title,\n",
    "        metadata={\"type\": \"title\", \"node_index\": 0, **md}\n",
    "    )\n",
    "    title_node.doc_id = f\"{md['PMID']}:0\"\n",
    "    all_nodes.append(title_node)\n",
    "\n",
    "    # Abstract nodes\n",
    "    abstract_doc = Document(\n",
    "        text=abstract,\n",
    "        metadata={\"type\": \"abstract\", **md}\n",
    "    )\n",
    "    abstract_doc.doc_id = str(md[\"PMID\"])\n",
    "\n",
    "    abstract_nodes = splitter.get_nodes_from_documents([abstract_doc])\n",
    "    for i, node in enumerate(abstract_nodes, start=1):\n",
    "        node.metadata[\"node_index\"] = i\n",
    "        all_nodes.append(node)\n",
    "\n",
    "# --------------------------\n",
    "# Save nodes both to Pinecone (for vector) and local docstore\n",
    "# --------------------------\n",
    "# print(\"Indexing nodes into Pinecone and persisting locally...\")\n",
    "\n",
    "# index = VectorStoreIndex(\n",
    "#     all_nodes,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=embed_model,\n",
    "#     show_progress=True\n",
    "# )\n",
    "\n",
    "# Persist docstore + metadata to disk\n",
    "        \n",
    "# --------------------------\n",
    "# Create a persistent docstore\n",
    "# --------------------------\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "docstore.add_documents(all_nodes)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    docstore=docstore\n",
    ")\n",
    "\n",
    "storage_context.persist(persist_dir=\"pubmed_nodes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Injecting Embedded Chunks into PineCone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e98acb0d8e3479abbd76b5015066f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92463109e904efaa3ca6975171ae5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4️ Store nodes in Pinecone on Cloud via LlamaIndex\n",
    "# --------------------------\n",
    "index = VectorStoreIndex([], storage_context=storage_context, embed_model=embed_model)\n",
    "if all_nodes:\n",
    "    index.insert_nodes(all_nodes, show_progress=True)\n",
    "else:\n",
    "    print(\"WARNING: No nodes to upsert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 493}},\n",
       " 'total_vector_count': 493,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Stats of Vector Index\n",
    "stats = pinecone_index.describe_index_stats()\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging: Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterCondition\n",
    "from openai import OpenAI\n",
    "import os, json\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def llm_extract_facets_simple(query_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Return STRICT JSON:\n",
    "    {\n",
    "      \"candidate_ingredients\": [str],\n",
    "      \"year_min\": int|null,\n",
    "      \"year_max\": int|null,\n",
    "      \"hints\": {\n",
    "        \"study_type\": [str],\n",
    "        \"species\": [str],\n",
    "        \"population\": str|null,\n",
    "        \"benefits\": [str],\n",
    "        \"diseases\": [str],\n",
    "        \"symptoms\": [str],\n",
    "        \"location\": str|null\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You extract minimal retrieval facets for PubMed-style search.\\n\"\n",
    "        \"Return STRICT JSON with EXACT keys: candidate_ingredients, year_min, year_max, hints.\\n\"\n",
    "        \"The 'hints' value MUST be a JSON OBJECT with EXACT keys:\\n\"\n",
    "        \"  study_type (list of strings),\\n\"\n",
    "        \"  species (list of strings),\\n\"\n",
    "        \"  population (string or null),\\n\"\n",
    "        \"  benefits (list of strings),\\n\"\n",
    "        \"  diseases (list of strings),\\n\"\n",
    "        \"  symptoms (list of strings),\\n\"\n",
    "        \"  location (string or null).\\n\"\n",
    "        \"Do NOT return 'hints' as an array. Use null for unknown scalars and [] for unknown lists.\\n\"\n",
    "        \"Return ONLY valid JSON. No extra keys.\"\n",
    "    )\n",
    "\n",
    "    # A tiny, explicit JSON shape example helps models obey the spec:\n",
    "    example = {\n",
    "        \"candidate_ingredients\": [\"cedarwood essential oil\"],\n",
    "        \"year_min\": 1997,\n",
    "        \"year_max\": 2023,\n",
    "        \"hints\": {\n",
    "            \"study_type\": [\"randomized controlled trial\", \"double-blind\"],\n",
    "            \"species\": [\"humans\"],\n",
    "            \"population\": \"patients with alopecia areata\",\n",
    "            \"benefits\": [\"hair regrowth\"],\n",
    "            \"diseases\": [\"alopecia areata\"],\n",
    "            \"symptoms\": [],\n",
    "            \"location\": None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    user = (\n",
    "        f\"Query: {query_text}\\n\"\n",
    "        \"Return only JSON in the exact schema shown below (keys and types must match):\\n\"\n",
    "        #f\"{json.dumps(example, ensure_ascii=False)}\"\n",
    "    )\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[{\"role\": \"system\", \"content\": system},\n",
    "                  {\"role\": \"user\", \"content\": user}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    try:\n",
    "        data = json.loads(resp.choices[0].message.content)\n",
    "    except Exception:\n",
    "        data = {}\n",
    "\n",
    "    # --- Minimal post-parse normalization (keeps you robust) ---\n",
    "    hints = data.get(\"hints\", {})\n",
    "    # If a list slipped through, coerce it to an object with best-effort buckets.\n",
    "    if isinstance(hints, list):\n",
    "        hints = _coerce_hints_list_to_object(hints)\n",
    "\n",
    "    out = {\n",
    "        \"candidate_ingredients\": [str(x) for x in data.get(\"candidate_ingredients\", []) if str(x).strip()],\n",
    "        \"year_min\": data.get(\"year_min\") if isinstance(data.get(\"year_min\"), int) else None,\n",
    "        \"year_max\": data.get(\"year_max\") if isinstance(data.get(\"year_max\"), int) else None,\n",
    "        \"hints\": {\n",
    "            \"study_type\": [str(x) for x in (hints.get(\"study_type\") or []) if str(x).strip()],\n",
    "            \"species\":    [str(x) for x in (hints.get(\"species\")    or []) if str(x).strip()],\n",
    "            \"population\": (hints.get(\"population\") if isinstance(hints.get(\"population\"), str) else None),\n",
    "            \"benefits\":   [str(x) for x in (hints.get(\"benefits\")   or []) if str(x).strip()],\n",
    "            \"diseases\":   [str(x) for x in (hints.get(\"diseases\")   or []) if str(x).strip()],\n",
    "            \"symptoms\":   [str(x) for x in (hints.get(\"symptoms\")   or []) if str(x).strip()],\n",
    "            \"location\":   (hints.get(\"location\") if isinstance(hints.get(\"location\"), str) else None),\n",
    "        },\n",
    "    }\n",
    "    return out\n",
    "\n",
    "    \n",
    "def llm_map_to_roots(candidates: list[str], allowed_roots: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Ask LLM to map candidate ingredient mentions to the canonical root_name(s) from allowed_roots.\n",
    "    Returns a unique list of chosen roots that exist in allowed_roots.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    system = (\n",
    "        \"Map ingredient mentions to canonical names from a provided list. \"\n",
    "        \"If no match, omit it. Return JSON: {\\\"roots\\\": [..canonical names..]}\"\n",
    "    )\n",
    "    user = (\n",
    "        \"Candidates: \" + json.dumps(candidates) + \"\\n\"\n",
    "        \"Allowed roots: \" + json.dumps(allowed_roots) + \"\\n\"\n",
    "        \"Return only JSON.\"\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    try:\n",
    "        data = json.loads(resp.choices[0].message.content)\n",
    "        roots = data.get(\"roots\", [])\n",
    "        # keep only valid ones in allowed list\n",
    "        valid = [r for r in roots if r in allowed_roots]\n",
    "        return sorted(set(valid))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def pinecone_filters_from_facets(roots: list[str], year_min: int|None, year_max: int|None):\n",
    "    fs = []\n",
    "    if roots:\n",
    "        # we indexed root + aliases in \"synonyms\"; this matches both\n",
    "        fs.append(MetadataFilter(key=\"synonyms\", operator=\"in\", value=[r.lower() for r in roots]))\n",
    "    if isinstance(year_min, int):\n",
    "        fs.append(MetadataFilter(key=\"published_year\", operator=\">=\", value=year_min))\n",
    "    if isinstance(year_max, int):\n",
    "        fs.append(MetadataFilter(key=\"published_year\", operator=\"<=\", value=year_max))\n",
    "    return MetadataFilters(filters=fs, condition=FilterCondition.AND) if fs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lc_set(x):\n",
    "    if isinstance(x, list): return {str(v).strip().lower() for v in x if v is not None}\n",
    "    if isinstance(x, str):  return {x.strip().lower()}\n",
    "    return set()\n",
    "\n",
    "def rerank_with_facets(candidates, hints: dict, weights=None, top_k=20):\n",
    "    \"\"\"\n",
    "    candidates: list[NodeWithScore] from vector retrieval\n",
    "    hints: dict (study_type, species, population, benefits, diseases, symptoms, location)\n",
    "    weights: per-field boosts (default small)\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            \"study_type\": 0.10,\n",
    "            \"species\": 0.08,\n",
    "            \"population\": 0.06,\n",
    "            \"benefits\": 0.06,\n",
    "            \"diseases\": 0.06,\n",
    "            \"symptoms\": 0.06,\n",
    "            \"location\": 0.05,\n",
    "        }\n",
    "\n",
    "    # normalize hints to lowercase sets/strings\n",
    "    h = {\n",
    "        \"study_type\": _lc_set(hints.get(\"study_type\", [])),\n",
    "        \"species\":    _lc_set(hints.get(\"species\", [])),\n",
    "        \"population\": (hints.get(\"population\") or \"\").strip().lower(),\n",
    "        \"benefits\":   _lc_set(hints.get(\"benefits\", [])),\n",
    "        \"diseases\":   _lc_set(hints.get(\"diseases\", [])),\n",
    "        \"symptoms\":   _lc_set(hints.get(\"symptoms\", [])),\n",
    "        \"location\":   (hints.get(\"location\") or \"\").strip().lower(),\n",
    "    }\n",
    "\n",
    "    rescored = []\n",
    "    for n in candidates:\n",
    "        base = float(n.score or 0.0)\n",
    "        md = n.node.metadata or {}\n",
    "\n",
    "        # list overlaps\n",
    "        def jaccard(a, b):\n",
    "            if not a or not b: return 0.0\n",
    "            inter = len(a & b); uni = len(a | b)\n",
    "            return inter / uni if uni else 0.0\n",
    "\n",
    "        st = jaccard(_lc_set(md.get(\"study_type\", [])), h[\"study_type\"])\n",
    "        sp = jaccard(_lc_set(md.get(\"species\", [])), h[\"species\"])\n",
    "        bf = jaccard(_lc_set(md.get(\"benefits\", [])), h[\"benefits\"])\n",
    "        ds = jaccard(_lc_set(md.get(\"diseases\", [])), h[\"diseases\"])\n",
    "        sy = jaccard(_lc_set(md.get(\"symptoms\", [])), h[\"symptoms\"])\n",
    "\n",
    "        # scalar contains\n",
    "        pop_hit = 1.0 if h[\"population\"] and isinstance(md.get(\"population\"), str) and h[\"population\"] in md[\"population\"].lower() else 0.0\n",
    "        loc_hit = 1.0 if h[\"location\"] and isinstance(md.get(\"location\"), str) and h[\"location\"] in md[\"location\"].lower() else 0.0\n",
    "\n",
    "        boost = (\n",
    "            weights[\"study_type\"] * st +\n",
    "            weights[\"species\"]    * sp +\n",
    "            weights[\"benefits\"]   * bf +\n",
    "            weights[\"diseases\"]   * ds +\n",
    "            weights[\"symptoms\"]   * sy +\n",
    "            weights[\"population\"] * pop_hit +\n",
    "            weights[\"location\"]   * loc_hit\n",
    "        )\n",
    "        rescored.append((base + boost, n))\n",
    "\n",
    "    rescored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [n for _, n in rescored[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "def make_minimal_hybrid(index, docstore, filters=None, vec_k=40, bm25_k=50, final_k=20):\n",
    "    vector_ret = index.as_retriever(similarity_top_k=vec_k, filters=filters)\n",
    "    bm25_ret   = BM25Retriever.from_defaults(docstore=docstore, similarity_top_k=bm25_k)\n",
    "    return QueryFusionRetriever(\n",
    "        retrievers=[vector_ret, bm25_ret],\n",
    "        mode=\"reciprocal_rerank\",\n",
    "        num_queries=1,\n",
    "        similarity_top_k=final_k,\n",
    "    )\n",
    "\n",
    "def _passes_simple_filters(md, roots_lc, year_min, year_max):\n",
    "    # roots via synonyms (you indexed all aliases in \"synonyms\")\n",
    "    if roots_lc:\n",
    "        syns = {str(x).strip().lower() for x in (md.get(\"synonyms\") or [])}\n",
    "        if not syns.intersection(set(roots_lc)):\n",
    "            return False\n",
    "    # year range\n",
    "    y = md.get(\"published_year\")\n",
    "    if isinstance(y, int):\n",
    "        if year_min is not None and y < year_min: return False\n",
    "        if year_max is not None and y > year_max: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_candidates_to_roots_via_alias(cands, alias2root):\n",
    "    roots = []\n",
    "    for c in cands:\n",
    "        key = str(c).strip().lower()\n",
    "        if key in alias2root:\n",
    "            roots.append(alias2root[key])\n",
    "    # unique & stable\n",
    "    return sorted(set(roots))\n",
    "\n",
    "def rag_retrieve(query_text, vec_top_k=50, final_top_k=20):\n",
    "    # 1) LLM → facets (your simple extractor)\n",
    "    raw = llm_extract_facets_simple(query_text)\n",
    "    print(raw)\n",
    "    # 2) map ingredient aliases → roots\n",
    "    roots = map_candidates_to_roots_via_alias(raw.get(\"candidate_ingredients\", []), ALIAS2ROOT)\n",
    "\n",
    "    # 3) build Pinecone filters (ingredient + year)\n",
    "    filters = pinecone_filters_from_facets(\n",
    "        roots=roots,\n",
    "        year_min=raw.get(\"year_min\"),\n",
    "        year_max=raw.get(\"year_max\")\n",
    "    )\n",
    "\n",
    "    # 4) HYBRID retrieve (vector uses filters; BM25 ignores them)\n",
    "    hybrid = make_minimal_hybrid(\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        filters=filters,\n",
    "        vec_k=vec_top_k,\n",
    "        bm25_k=max(vec_top_k, 50),  # small bump to keep BM25 recall decent\n",
    "        final_k=max(final_top_k, 20)\n",
    "    )\n",
    "    candidates = hybrid.retrieve(query_text)\n",
    "\n",
    "    # 4b) (Optional) local hard filter so BM25 strays get trimmed\n",
    "    roots_lc = [r.lower() for r in roots] if roots else []\n",
    "    y_min, y_max = raw.get(\"year_min\"), raw.get(\"year_max\")\n",
    "    if roots_lc or (y_min is not None) or (y_max is not None):\n",
    "        candidates = [\n",
    "            c for c in candidates\n",
    "            if _passes_simple_filters(c.node.metadata, roots_lc, y_min, y_max)\n",
    "        ]\n",
    "\n",
    "    # If still empty, relax filters (rare). Try vector-only, no filters.\n",
    "    if not candidates:\n",
    "        vret = index.as_retriever(similarity_top_k=vec_top_k)  # no filters\n",
    "        candidates = vret.retrieve(query_text)\n",
    "\n",
    "    # 5) facet-aware soft re-rank (your existing function)\n",
    "    hints = raw.get(\"hints\") if isinstance(raw.get(\"hints\"), dict) else {}\n",
    "    reranked = rerank_with_facets(candidates, hints, top_k=final_top_k)\n",
    "\n",
    "    used_filters = {\n",
    "        \"synonyms\": roots,\n",
    "        \"year_min\": raw.get(\"year_min\"),\n",
    "        \"year_max\": raw.get(\"year_max\"),\n",
    "    }\n",
    "    return reranked, used_filters, hints\n",
    "\n",
    "# NOT HYBRID\n",
    "# NOT HYBRID\n",
    "# NOT HYBRID\n",
    "# def rag_retrieve(query_text, vec_top_k=50, final_top_k=20):\n",
    "#     # 1) LLM → facets\n",
    "#     raw = llm_extract_facets_simple(query_text)\n",
    "\n",
    "#     # 2) map ingredient aliases → roots\n",
    "#     roots = map_candidates_to_roots_via_alias(raw.get(\"candidate_ingredients\", []), ALIAS2ROOT)\n",
    "\n",
    "#     # 3) build ultra-simple pinecone filters (ingredient + year)\n",
    "#     filters = pinecone_filters_from_facets(roots, raw.get(\"year_min\"), raw.get(\"year_max\"))\n",
    "\n",
    "#     # 4) vector retrieve (first with filters, fall back without if 0)\n",
    "#     vret = index.as_retriever(similarity_top_k=vec_top_k, filters=filters)\n",
    "#     cands = vret.retrieve(query_text)\n",
    "#     if not cands and filters is not None:\n",
    "#         # fallback: remove filters entirely, keep recall\n",
    "#         vret = index.as_retriever(similarity_top_k=vec_top_k)\n",
    "#         cands = vret.retrieve(query_text)\n",
    "\n",
    "#     # 5) facet-aware re-rank (soft boosts)\n",
    "#     hints = raw.get(\"hints\") if isinstance(raw.get(\"hints\"), dict) else {}\n",
    "#     reranked = rerank_with_facets(cands, hints, top_k=final_top_k)\n",
    "\n",
    "#     # return also a compact view of the (used) filters\n",
    "#     used_filters = {\"synonyms\": roots, \"year_min\": raw.get(\"year_min\"), \"year_max\": raw.get(\"year_max\")}\n",
    "#     return reranked, used_filters, raw.get(\"hints\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidate_ingredients': ['cedarwood oil'], 'year_min': 1997, 'year_max': 2023, 'hints': {'study_type': ['randomized controlled trial', 'double-blind study'], 'species': ['Homo sapiens'], 'population': None, 'benefits': ['improvement in hair regrowth', 'reduction in hair loss'], 'diseases': ['alopecia areata'], 'symptoms': ['hair loss'], 'location': None}}\n",
      "used_filters: {'synonyms': ['cedarwood'], 'year_min': 1997, 'year_max': 2023}\n",
      "hints: {'study_type': ['randomized controlled trial', 'double-blind study'], 'species': ['Homo sapiens'], 'population': None, 'benefits': ['improvement in hair regrowth', 'reduction in hair loss'], 'diseases': ['alopecia areata'], 'symptoms': ['hair loss'], 'location': None}\n",
      "Number of hits: 21\n",
      "Score 0.0492 | PMID 9828867 | abstract | year 1998 | root Cedarwood\n",
      "Score 0.0479 | PMID 9828867 | abstract | year 1998 | root Cedarwood\n",
      "Score 0.0328 | PMID 9828867 | title | year 1998 | root Cedarwood\n",
      "Score 0.0426 | PMID 12805340 | abstract | year 2003 | root Cedarwood\n",
      "Score 0.0402 | PMID 12805340 | abstract | year 2003 | root Cedarwood\n",
      "Score 0.0292 | PMID 12805340 | title | year 2003 | root Cedarwood\n",
      "Score 0.0412 | PMID 26910133 | abstract | year 2016 | root Cedarwood\n",
      "Score 0.0400 | PMID 26910133 | abstract | year 2016 | root Cedarwood\n",
      "Score 0.0371 | PMID 17191830 | abstract | year 2004 | root Cedarwood\n",
      "Score 0.0358 | PMID 32439404 | abstract | year 2020 | root Cedarwood\n",
      "Score 0.0329 | PMID 22430697 | abstract | year 2012 | root Cedarwood\n",
      "Score 0.0329 | PMID 22430697 | abstract | year 2012 | root Cedarwood\n",
      "Score 0.0299 | PMID 32439404 | abstract | year 2020 | root Cedarwood\n",
      "Score 0.0263 | PMID 26910133 | title | year 2016 | root Cedarwood\n",
      "Score 0.0252 | PMID 17191830 | abstract | year 2004 | root Cedarwood\n",
      "Score 0.0240 | PMID 15080642 | abstract | year 2004 | root Cedarwood\n",
      "Score 0.0229 | PMID 15080642 | abstract | year 2004 | root Cedarwood\n",
      "Score 0.0214 | PMID 17590010 | abstract | year 2007 | root Cedarwood\n",
      "Score 0.0205 | PMID 32439404 | title | year 2020 | root Cedarwood\n",
      "Score 0.0201 | PMID 22928313 | abstract | year 2012 | root Cedarwood\n",
      "Score 0.0143 | PMID 17191830 | title | year 2004 | root Cedarwood\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever for similarity search\n",
    "#retriever = index.as_retriever(similarity_top_k=5)  # retrieve top 5 similar chunks\n",
    "query_text = \"Show human randomized, double-blind clinical trials since the late 1990s where cedarwood oil was used topically to treat alopecia areata and report primary clinical outcomes.\"\n",
    "hits, used_filters, hints = rag_retrieve(\n",
    "    query_text,\n",
    "    vec_top_k=50,\n",
    "    final_top_k=25\n",
    ")\n",
    "\n",
    "print(\"used_filters:\", used_filters)\n",
    "print(\"hints:\", hints)\n",
    "print(\"Number of hits:\", len(hits))\n",
    "for r in hits:\n",
    "    md = r.node.metadata\n",
    "    print(f\"Score {r.score:.4f} | PMID {md.get('PMID')} | {md.get('type')} | year {md.get('published_year')} | root {md.get('root_name')}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "# results = retriever.retrieve(query_text)\n",
    "# for res in results:\n",
    "#     print(\"Score:\", res.score)\n",
    "#     print(\"Text:\", res.node.text)\n",
    "#     print(\"PMID:\", res.node.metadata.get(\"PMID\"))\n",
    "#     print(\"Type:\", res.node.metadata.get(\"type\"))\n",
    "#     print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12805340\n",
    "#\"I’m looking for solid human studies on cedar leaf oil to help with anxiety during radiotherapy. Can you pull primary-outcome evidence since about 2000, inhalation use only, and ignore chemistry/extraction papers?\"\n",
    "\n",
    "#12805340\n",
    "#\"Find human randomized, double-blind studies where cedarwood or cedar leaf oil was delivered by inhalation to patients undergoing radiotherapy to reduce anxiety, from ~2000 onward. Ignore extraction or chemistry papers.\"\n",
    "\n",
    "#9828867\n",
    "#\"Show human randomized, double-blind clinical trials since the late 1990s where cedarwood oil was used topically to treat alopecia areata and report primary clinical outcomes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging: Reconstruction the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reconstruct a paper from nodes\n",
    "def reconstruct_paper(all_nodes, pmid):\n",
    "    # Filter nodes belonging to this paper\n",
    "    paper_nodes = [node for node in all_nodes if str(node.metadata.get(\"PMID\")) == str(pmid)]\n",
    "    \n",
    "    # Sort nodes by node_index\n",
    "    paper_nodes = sorted(paper_nodes, key=lambda x: x.metadata.get(\"node_index\", 0))\n",
    "    print(\"Noumber of Nodes:\",len(paper_nodes))\n",
    "    # Concatenate the text\n",
    "    full_text = \"\\n\".join([node.text for node in paper_nodes])\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# Example usage\n",
    "pmid_to_reconstruct = restructured_records[0]['metadata']['PMID']\n",
    "full_paper_text = reconstruct_paper(all_nodes, pmid_to_reconstruct)\n",
    "\n",
    "print(\"Reconstructed Paper Text:\")\n",
    "print(full_paper_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Delete PineCone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# INDEX_NAME = \"pubmed-abstracts\"\n",
    "\n",
    "# # Initialize Pinecone client\n",
    "# client = Pinecone(api_key=os.getenv(\"PINECONE_API\"))\n",
    "\n",
    "# try:\n",
    "#     client.delete_index(name=INDEX_NAME)\n",
    "#     print(\"Index deleted\")\n",
    "# except:\n",
    "#     print(\"Data base is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opitional: Check available Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = client.list_indexes()\n",
    "# print(f\"Available indexes: {indexes.names()}\")\n",
    "# print(f\"Current index name: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search Retrival Pipeline directly from Pinecone\n",
    "Note: Make Sure to restart the kernal before you run the below cell to ensure that data is not being retrived from local- memory/in-memory/RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "#from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "# Just point to the folder where you persisted\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"pubmed_nodes\")\n",
    "\n",
    "# Now access your persisted documents\n",
    "docstore = storage_context.docstore\n",
    "print(\"Number of documents:\", len(docstore.docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API\"))\n",
    "pinecone_index = pc.Index(INDEX_NAME)\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Create vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# Create BM25 retriever for keyword-based search\n",
    "# Ensure you have the documents loaded in memory for BM25\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "# Create hybrid retriever using QueryFusionRetriever\n",
    "# This combines results from both retrievers\n",
    "# hybrid_retriever = QueryFusionRetriever(\n",
    "#     retrievers=[vector_retriever, bm25_retriever],\n",
    "#     retriever_weights=[0.5, 0.5],  # Equal weight to both retrievers\n",
    "#     llm=MockLLM(),  # Use MockLLM to avoid needing OpenAI API key\n",
    "#     use_async=False,\n",
    "#     #mode=\"reciprocal_rerank\",\n",
    "# )\n",
    "\n",
    "# Perform hybrid search\n",
    "query = \"Which analytical method was used to photosynthetic tissues?\"\n",
    "results = hybrid_retriever.retrieve(query)\n",
    "\n",
    "# Display results\n",
    "for res in results:\n",
    "    print(\"Score:\", res.score)\n",
    "    print(\"Text:\", res.node.text)\n",
    "    print(\"PMID:\", res.node.metadata.get(\"PMID\"))\n",
    "    print(\"Type:\", res.node.metadata.get(\"type\"))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
